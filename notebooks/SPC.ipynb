{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import h5py\n",
    "from io import StringIO\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "import itertools\n",
    "import xlsxwriter as Excel\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from numba import jit\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"samples\"\n",
    "#converts to df to binary dataframe, 0 representing that the previous value was 0, and 1 indicating a nonzero value\n",
    "def df_to_binary(df_expression):    \n",
    "    temp = df_expression.copy()\n",
    "    num_rows = len(temp)\n",
    "    for c in range(len(df_expression.columns)):\n",
    "        nonzero_inds = df_expression.iloc[:,c].to_numpy().nonzero()[0]\n",
    "        temp_col = np.zeros(num_rows)\n",
    "        temp_col[nonzero_inds] = 1\n",
    "        temp.iloc[:,c] = temp_col\n",
    "    return(temp)\n",
    "\n",
    "\n",
    "\n",
    "#INPUTS:\n",
    "#  file_list: list of csv filenames containing expression data. Will concatenate these matrices together and compute distances\n",
    "def preprocess_samples(file_list):\n",
    "    sample_source = []\n",
    "    # Merge all of the files into one dataframe\n",
    "    df_expression = pd.read_csv(os.path.join(data_path,file_list[0]),index_col = 0,sep=\"\\t\")\n",
    "    sample_source.extend(np.repeat(file_list[0],len(df_expression.columns)))\n",
    "    for filename in file_list[1:]:\n",
    "        temp = pd.read_csv(os.path.join(data_path,filename),index_col = 0, sep = \"\\t\")\n",
    "        sample_source.extend(np.repeat(filename,len(temp.columns)))\n",
    "        df_expression = df_expression.merge(temp,right_index=True,left_index=True)\n",
    "    \n",
    "    df_expression.columns = pd.Series(df_expression.columns)\n",
    "    df_expression.index = pd.Series(df_expression.index)\n",
    "    \n",
    "    return(df_expression,sample_source)    \n",
    "\n",
    "def hierarchical_clustering(df,title,method=\"average\",metric=\"euclidean\"):\n",
    "    clustering = shc.linkage(df, method=method, metric=metric)\n",
    "    plt.figure(figsize=(25, 20))  \n",
    "    plt.title(title)  \n",
    "\n",
    "    dend = shc.dendrogram(clustering) \n",
    "    print(\"Cophenet = \" + str(cophenet(clustering, pdist(df))[0]))\n",
    "    return(clustering)\n",
    "\n",
    "#INPUTS: \n",
    "#  h5filepath: path where the full matrix is stored with meta and expression data\n",
    "#  subsamples: a pandas DataFrame that must contain a column (foreign key) called SampleGeoAccession\n",
    "#OUTPUT\n",
    "#  merged dataframe of length(subsamples) merged with relevant meta/Sample values for cluster analysis\n",
    "def h5_sample_meta_lookup(h5filepath,subsamples):\n",
    "    f = h5py.File(h5filepath, 'r')\n",
    "    full_data = pd.DataFrame({\"Description\":f[(\"meta/Sample_description\")], \n",
    "                          \"Characteristics\":f[(\"meta/Sample_characteristics_ch1\")],\n",
    "                          \"SampleGeoAccession\":f[(\"meta/Sample_geo_accession\")],\n",
    "                          \"Series ID\":f[(\"meta/Sample_series_id\")],\n",
    "                          \"Molecule\":f[\"meta/Sample_molecule_ch1\"],\n",
    "                          \"Source Name\":f[\"meta/Sample_title\"]})\n",
    "    f.close()\n",
    "    full_data[\"SampleGeoAccession\"] = full_data[\"SampleGeoAccession\"].apply(lambda x : x.decode(\"utf-8\"))\n",
    "    cluster_meta = subsamples.merge(full_data,on=\"SampleGeoAccession\",how=\"left\")\n",
    "    cluster_meta[\"Sample_Label\"] = pd.Series(sample_source).str.replace(\".csv\",\"\").str.replace(\"_expression\",\" \")\n",
    "    cluster_meta[\"Series ID\"] = cluster_meta[\"Series ID\"].str.decode(\"utf-8\").str.replace(\"Xx\",\"\").str.replace(\"xX\",\"\")\n",
    "    cluster_meta[\"Molecule\"] = cluster_meta[\"Molecule\"].str.decode(\"utf-8\")\n",
    "    cluster_meta[\"Source Name\"] = cluster_meta[\"Source Name\"].str.decode(\"utf-8\")\n",
    "    cluster_meta[\"Description\"] = cluster_meta[\"Description\"].str.decode(\"utf-8\").str.replace(\"Xx\",\"\").str.replace(\"xX\",\"\")\n",
    "    cluster_meta[\"Characteristics\"] = cluster_meta[\"Characteristics\"].str.decode(\"utf-8\").str.replace(\"Xx\",\"\").str.replace(\"xX\",\"\")\n",
    "    return(cluster_meta)\n",
    "\n",
    "def purity_to_excel(cluster_metadata_metrics, H0, outfile):\n",
    "    workbook = Excel.Workbook(\"tables/\" +outfile +\".xlsx\")\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    cluster_format = workbook.add_format({'bold': True, 'align':'center'})\n",
    "    center = workbook.add_format({'align': 'center'})\n",
    "\n",
    "    # Add a format. Light red fill with dark red text.\n",
    "    format1 = workbook.add_format({'bg_color': '#FFC7CE',\n",
    "                                   'font_color': '#9C0006'})\n",
    "\n",
    "    # Add a format. Green fill with dark green text.\n",
    "    format2 = workbook.add_format({'bg_color': '#C6EFCE',\n",
    "                                   'font_color': '#006100'})\n",
    "\n",
    "    worksheet.write('A1', \"Cluster ID\",cluster_format)\n",
    "    worksheet.write('B1', \"Cluster Size\",center)\n",
    "    worksheet.write('C1', \"Dominant Molecule\",center)\n",
    "    worksheet.write('D1', \"Molecule Purity\",center)\n",
    "    worksheet.write('E1', \"Dominant Series\",center)\n",
    "    worksheet.write('F1', \"Series Purity\",center)\n",
    "    worksheet.write('G1', \"Dominant Label\",center)\n",
    "    worksheet.write('H1', \"Label Purity\",center)\n",
    "    worksheet.write('I1', \"Null Hypothesis Value\",center)\n",
    "\n",
    "    worksheet.set_column(1, 7, 15)\n",
    "\n",
    "    num_keys = len(list(cluster_metadata_metrics.keys()))\n",
    "\n",
    "    j = 1\n",
    "    for i in np.sort(list(cluster_metadata_metrics.keys())):\n",
    "        worksheet.write(j,0,i,center)\n",
    "        worksheet.write(j,1,cluster_metadata_metrics[i][\"Size\"],center)\n",
    "        worksheet.write(j,2,list(cluster_metadata_metrics[i][\"Molecule Purity\"].keys())[0],center)\n",
    "        worksheet.write(j,3,list(cluster_metadata_metrics[i][\"Molecule Purity\"].values())[0],center)\n",
    "        worksheet.write(j,4,list(cluster_metadata_metrics[i][\"Series Purity\"].keys())[0],center)\n",
    "        worksheet.write(j,5,list(cluster_metadata_metrics[i][\"Series Purity\"].values())[0],center)\n",
    "        worksheet.write(j,6,list(cluster_metadata_metrics[i][\"Label Purity\"].keys())[0],center)\n",
    "        worksheet.write(j,7,list(cluster_metadata_metrics[i][\"Label Purity\"].values())[0],center)\n",
    "        worksheet.write(j,8,H0[list(cluster_metadata_metrics[i][\"Label Purity\"].keys())[0]],center)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Write a conditional format over a range.\n",
    "        worksheet.conditional_format('H'+str(j+1), {'type': 'cell',\n",
    "                                             'criteria': '>=',\n",
    "                                             'value': H0[list(cluster_metadata_metrics[i][\"Label Purity\"].keys())[0]],\n",
    "                                             'format': format2})\n",
    "\n",
    "        # Write another conditional format over the same range.\n",
    "        worksheet.conditional_format('H'+str(j+1), {'type': 'cell',\n",
    "                                             'criteria': '<',\n",
    "                                             'value': H0[list(cluster_metadata_metrics[i][\"Label Purity\"].keys())[0]],\n",
    "                                             'format': format1})\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "\n",
    "def get_purity(cluster_assignments, df_expression_trans):\n",
    "    cluster_dict = {\"SampleGeoAccession\":[],\"ClusterAssignment\":[]}\n",
    "    cluster_dict[\"SampleGeoAccession\"] = df_expression_trans.index\n",
    "    cluster_dict[\"ClusterAssignment\"] = cluster_assignments\n",
    "    cluster_info = pd.DataFrame(cluster_dict)\n",
    "    cluster_metadata = h5_sample_meta_lookup(\"D:\\\\548project\\\\human_matrix.h5\",cluster_info).sort_values(\"ClusterAssignment\")\n",
    "    \n",
    "    #hypothesized values of expected sample frequencies assuming no separation (based on observed proportion of sample labels)\n",
    "    H0 = {}\n",
    "    for samp in cluster_metadata.Sample_Label:\n",
    "        H0[samp] = len(cluster_metadata[cluster_metadata.Sample_Label == samp])/len(cluster_metadata)\n",
    "    \n",
    "    cluster_metadata_metrics = {}\n",
    "    for c in range(1,k+1):\n",
    "        cluster_metadata_metrics[c] = {\"Size\":len(cluster_metadata.loc[cluster_metadata.ClusterAssignment == c]), \n",
    "                                       \"Molecule Purity\":{},\"Series Purity\":{},\"Label Purity\":{}}\n",
    "        series_counts = cluster_metadata.groupby('ClusterAssignment')[\"Series ID\"].value_counts()\n",
    "        molecule_counts = cluster_metadata.groupby('ClusterAssignment')[\"Molecule\"].value_counts()\n",
    "        label_counts = cluster_metadata.groupby('ClusterAssignment')[\"Sample_Label\"].value_counts()\n",
    "\n",
    "        series_purity_key = pd.Series(series_counts[c][series_counts[c] == series_counts[c].max()]).index[0]\n",
    "        series_purity_val = series_counts[c].max()/float(series_counts[c].sum())\n",
    "        cluster_metadata_metrics[c][\"Series Purity\"][series_purity_key] = round(series_purity_val,3)\n",
    "\n",
    "        molecule_purity_key = molecule_counts[c][molecule_counts[c] == molecule_counts[c].max()].index[0]\n",
    "        molecule_purity_val = molecule_counts[c].max()/float(molecule_counts[c].sum())\n",
    "        cluster_metadata_metrics[c][\"Molecule Purity\"][molecule_purity_key] = round(molecule_purity_val,3)\n",
    "\n",
    "        label_purity_key = label_counts[c][label_counts[c] == label_counts[c].max()].index[0]\n",
    "        label_purity_val = label_counts[c].max()/float(label_counts[c].sum())\n",
    "        cluster_metadata_metrics[c][\"Label Purity\"][label_purity_key] = round(label_purity_val,3)\n",
    "    purity_to_excel(cluster_metadata_metrics,H0,cluster_title+\" purity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"ALPHA_expression.csv\",\"beta_expression.csv\",\"PANC1_expression.csv\"]\n",
    "df_expression_raw,sample_source = preprocess_samples(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expression_norm = (df_expression_raw - df_expression_raw.mean())/(df_expression_raw.std())\n",
    "method=\"average\"\n",
    "metric=\"euclidean\"\n",
    "#df_expression_trans = np.transpose(df_expression_norm)\n",
    "df_expression_trans = np.transpose(df_expression_raw)\n",
    "#df_expression_trans = np.transpose(df_to_binary(df_expression_trans))\n",
    "dataTitle=\"Normalized\"\n",
    "cluster_title = dataTitle +\" Data (Filtered) Liver vs. HEPG2 cell \"+method+\"+\"+metric\n",
    "df_expression_trans = np.transpose(df_expression_norm)\n",
    "genes = list(df_expression_trans.columns)\n",
    "samples = list(df_expression_trans.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(lst, item):\n",
    "    return [i for i, x in enumerate(lst) if x == item]\n",
    "\n",
    "def eHKnn(distance,K):\n",
    "    #Create the nodenext matrix which stores the neighbor in the neighborhood\n",
    "    #of size K. This needs to be supplemented by the nodes linked by the Minimum\n",
    "    #Spanning Tree. See pre-print\n",
    "    N=distance.shape[0]\n",
    "    nodenext=[]\n",
    "    Rank=np.argsort(distance)\n",
    "    Rank=Rank[:,:K+1]\n",
    "    r_ = np.zeros( (N,K), dtype=int)\n",
    "    for i in range(N):\n",
    "        rank = list(Rank[i])\n",
    "        rank.remove(i)\n",
    "        r_[i] = rank[:K]\n",
    "    Rank =  r_\n",
    "    for i in range(N):\n",
    "        e=[]\n",
    "        for j in Rank[i]:\n",
    "            if i in Rank[j]:\n",
    "                e.append(j)\n",
    "        nodenext.append(e)\n",
    "    return nodenext\n",
    "\n",
    "def eHK(link,nodenext):\n",
    "    #Extended Hoshen-Kopelman implementation. See pre-print\n",
    "\n",
    "    N=len(nodenext)\n",
    "    nodel=10*N*np.ones(N,dtype=int)\n",
    "    label_counter=0\n",
    "    nodelp=np.array([],dtype=int)\n",
    "    for i in range(N):\n",
    "        if (np.array(link[i])==0).all():\n",
    "            nodel[i]=label_counter\n",
    "            nodelp=np.append(nodelp,label_counter)\n",
    "            label_counter+=1\n",
    "        else:\n",
    "            idx = duplicates(link[i],1) #where links are\n",
    "            t=[nodel[ nodenext[i][j] ] for j in idx]\n",
    "            t=np.array(t)\n",
    "            if (t==10*N).all(): # all unlabeled?\n",
    "                nodel[i]=label_counter\n",
    "                nodelp=np.append(nodelp,label_counter)\n",
    "                label_counter+=1\n",
    "            else:\n",
    "                w=[]\n",
    "                for index in range(len(t)):\n",
    "                    if t[index]!=10*N:\n",
    "                        w.append(index)\n",
    "                idx_ = np.array([nodenext[i][j] for j in idx])\n",
    "                z = nodelp[nodel[ idx_[w] ] ]\n",
    "                min_ = np.amin(z)\n",
    "\n",
    "                nodel[i] = min_\n",
    "                a = nodel[ idx_[w] ]\n",
    "                nodelp[a]=min_\n",
    "\n",
    "    # sequentialize part 1: re-order nodelp\n",
    "    for y in range(len(nodelp)):\n",
    "        n = y\n",
    "        while (nodelp[n]<n):\n",
    "            n=nodelp[n]\n",
    "        nodelp[y]=n\n",
    "    # sequentialize part 2: get rid of the gaps\n",
    "    un = np.unique(nodelp)\n",
    "    for i in range(len(un)-1):\n",
    "        while un[i+1]-un[i] !=1:\n",
    "            idx = np.where(nodelp==un[i+1])[0]\n",
    "            nodelp[idx] -= 1\n",
    "            un = np.unique(nodelp)\n",
    "\n",
    "    # rename the labels with their root\n",
    "    for i in range( len(nodelp) ):\n",
    "        nodel[nodel==i]=nodelp[i]\n",
    "\n",
    "    return nodel\n",
    "\n",
    "\n",
    "\n",
    "def cHKlons(nodenext,G):\n",
    "    # This function serves to perform the consensus final cluster solution\n",
    "    # using the Spin Spin correlation matrix G\n",
    "    link=[]\n",
    "    N=G.shape[0]\n",
    "    for i in range(N):\n",
    "        neighbors=nodenext[i]\n",
    "        e=[]\n",
    "        for j in neighbors:\n",
    "            if (G[i,j]>0.5):\n",
    "                e.append(1)\n",
    "            else:\n",
    "                e.append(0)\n",
    "        idx=np.argmax(G[i,neighbors])\n",
    "        e[idx]=1\n",
    "        link.append(e)\n",
    "    # make sure the neighbor with the highest correlation is linked both ways\n",
    "    for i in range(N):\n",
    "        idx = duplicates(link[i],1)\n",
    "        for f in idx:\n",
    "            X = nodenext[i][f]\n",
    "            Y = duplicates(nodenext[X],i)\n",
    "            Y = Y[0]\n",
    "            link[X][Y] = 1\n",
    "    return link\n",
    "\n",
    "\n",
    "def kron(i, j):\n",
    "    #kronecker delta\n",
    "    if i == j:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def twopc(S,cij):\n",
    "    #two point connectedness\n",
    "    classes=np.unique(S)\n",
    "    for label in classes:\n",
    "        neighbors=duplicates(S,label)\n",
    "        for node in neighbors:\n",
    "            cij[node,neighbors]+=1\n",
    "    return cij\n",
    "\n",
    "def Hs(S, J, nodenext):\n",
    "   # Hamiltonian Energy\n",
    "    E=0\n",
    "    N = len(S)\n",
    "    for i in range(N):\n",
    "        for j in nodenext[i]:\n",
    "            E += J[i, j]*(1-kron(S[i], S[j]))\n",
    "    return E/N\n",
    "\n",
    "def magnetization(S, q):\n",
    "    N=len(S)\n",
    "    nmax = np.amax(np.bincount(S))\n",
    "    return (q*nmax-N)/((q-1)*N)\n",
    "\n",
    "def runz(S,f,mcmc,nodenext,J,t,q,K):\n",
    "    np.random.seed(0)\n",
    "    def flip(S, q):\n",
    "        # Flip clusters labels after Monte Carlo steps\n",
    "        c = np.unique(S)  # find unique labels\n",
    "        new_c = np.random.randint(0, q, len(c))  #gen new spins for clusters\n",
    "        conv = dict(zip(c, new_c))  # use dic to assign new spins to clusters\n",
    "        return np.vectorize(conv.get)(S)\n",
    "\n",
    "    def eHKlons(nodenext,T,J,S):\n",
    "        # Create link matrix, which stores the edges activation status\n",
    "        link=[]\n",
    "        N = len(nodenext)\n",
    "        for i in range(N):\n",
    "            e=[]\n",
    "            for j in nodenext[i]:\n",
    "                if (1-np.exp(-J[i,j]*kron(S[i],S[j])/T)>np.random.uniform() ):\n",
    "                    e.append(1)\n",
    "                else:\n",
    "                    e.append(0)\n",
    "            link.append(e)\n",
    "        return link\n",
    "\n",
    "    N=len(S)\n",
    "    forget=int(f*mcmc)\n",
    "    m=np.zeros(mcmc)\n",
    "    cij=np.zeros((N,N))\n",
    "    \n",
    "    for i in range(forget):\n",
    "        # the number of SW steps that allow the system to reach thermal eq\n",
    "        S1=S\n",
    "        E1=Hs(S1,J,nodenext)\n",
    "        LinkSnode = eHKlons(nodenext,t,J,S)\n",
    "        S = eHK(LinkSnode,nodenext)\n",
    "        S = flip(S, q)\n",
    "        E2 = Hs(S,J,nodenext)\n",
    "        if E2 >= E1:\n",
    "            if np.exp(- E2 / t) < np.random.uniform() :\n",
    "                S=S1\n",
    "\n",
    "    for i in range(mcmc):\n",
    "        #  Actual SW steps we keep\n",
    "        S1=S\n",
    "        E1=Hs(S1,J,nodenext)\n",
    "        LinkSnode = eHKlons(nodenext,t,J,S)\n",
    "        S = eHK(LinkSnode,nodenext)\n",
    "        S = flip(S, q)\n",
    "        E2 = Hs(S,J,nodenext)\n",
    "        if E2 >= E1:\n",
    "            if np.exp(- E2 / t) < np.random.uniform() :\n",
    "                S=S1\n",
    "                E2 = E1\n",
    "\n",
    "        cij=twopc(S,cij)\n",
    "        m[i]=magnetization(S,q)\n",
    "    # Compute thermodynamic averages here \n",
    "    mbar = np.average(m)  #  Average magnetization\n",
    "    su = N*np.var(m)/t #  Magnetic Susceptibility\n",
    "    return su, mbar, cij, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is coming from \"dataframe.name\".to_numpy()\n",
    "# nc is the cluster number\n",
    "# x is the iteration times. Example uses 60.\n",
    "def SPC(data,nc,x):\n",
    "    \n",
    "    # number of genes in this data\n",
    "    N = data.shape[0]\n",
    "    T = np.linspace(1e-6,.3,num=x,endpoint=True)\n",
    "    \n",
    "    distance = euclidean_distances(data)\n",
    "    \n",
    "    # number of mcmc steps\n",
    "    mcmc = 200\n",
    "    \n",
    "    # Determine the Graph, and its minimal spanning tree\n",
    "    Tree=nx.minimum_spanning_tree(nx.from_numpy_matrix(distance))\n",
    "    # Determine the neighborhood and add the Minimal Spanning Tree edges on top of it\n",
    "    \n",
    "    #Create the nodenext matrix which stores the neighbor in the neighborhood\n",
    "    # of size K. This needs to be supplemented by the nodes linked by the Minimum\n",
    "    # Spanning Tree. \n",
    "    K=10\n",
    "    nodenext = eHKnn(distance, K)\n",
    "    \n",
    "    #  Add the edges in the minimal spanning tree not in nodenext\n",
    "    mst_edges = list( Tree.edges())\n",
    "    for i in mst_edges:\n",
    "        node = i[0]\n",
    "        if i[1] not in nodenext[node]:\n",
    "            nodenext[node].append(i[1])\n",
    "            nodenext[node] = sorted(nodenext[node])\n",
    "            nodenext[i[1]].append(node)\n",
    "            nodenext[i[1]] =  sorted(nodenext[i[1]])\n",
    "    \n",
    "    # need average number of neighbors khat, and the local length scale a\n",
    "    khat = 0\n",
    "    a = 0\n",
    "    alpha = 4\n",
    "    \n",
    "    for i in nodenext:\n",
    "        khat+= len(i)\n",
    "        khat = khat / N\n",
    "        \n",
    "    for i in range(N):\n",
    "        a+=sum(distance[i,nodenext[i]])\n",
    "        a = alpha * a / (khat*N)\n",
    "    \n",
    "    # Interaction Strength\n",
    "    n = 2\n",
    "    J = (1 / khat) * np.exp(-( (n-1)/n ) * ( distance / a)**n)\n",
    "    \n",
    "    # How many mcmc steps are forgotten for every temperature t\n",
    "    f_=0.5\n",
    "\n",
    "    # The initial spin configuration S_0 for all temperatures\n",
    "    S=np.ones(N, dtype=int)\n",
    "    \n",
    "    #SPC runs sequentially but every temperatures are ran in parallel\n",
    "    results = Parallel(n_jobs=5)(delayed( runz )(S,f_,mcmc,nodenext,J,T[y],nc,K) for y in range(x))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 8 to array axis with dimension 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ec16a9cbfd2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_expression_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSPC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-2e1bebb33a73>\u001b[0m in \u001b[0;36mSPC\u001b[1;34m(data, nc, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Spanning Tree.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mnodenext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meHKnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#  Add the edges in the minimal spanning tree not in nodenext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3b2790d9e002>\u001b[0m in \u001b[0;36meHKnn\u001b[1;34m(distance, K)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mrank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mr_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mRank\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mr_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot copy sequence with size 8 to array axis with dimension 10"
     ]
    }
   ],
   "source": [
    "data1 = df_expression_trans.iloc[1:10,:].to_numpy()\n",
    "a = SPC(data1,20,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 17, 18, 18, 18, 18,\n",
       "       17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 17,\n",
       "       17, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[59][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example which works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'blobs'\n",
    "blob = datasets.make_blobs(n_samples=500,\n",
    "                           cluster_std=[0.25,0.5,1],\n",
    "                            random_state=0, n_features=500,shuffle=True)\n",
    "data = blob[0]\n",
    "N = data.shape[0]\n",
    "T=np.linspace(1e-6,.3,num=60,endpoint=True)\n",
    "K = 10\n",
    "q = 20\n",
    "alpha = 4\n",
    "distance = euclidean_distances(data)\n",
    "\n",
    "\n",
    "mcmc = 200\n",
    "\n",
    "k = len(T)\n",
    "\n",
    "Tree=nx.minimum_spanning_tree(nx.from_numpy_matrix(distance))\n",
    "\n",
    "nodenext = eHKnn(distance, K)\n",
    "mst_edges = list( Tree.edges())\n",
    "\n",
    "for i in mst_edges:\n",
    "    node = i[0]\n",
    "    if i[1] not in nodenext[node]:\n",
    "        nodenext[node].append(i[1])\n",
    "        nodenext[node] = sorted(nodenext[node])\n",
    "        nodenext[i[1]].append(node)\n",
    "        nodenext[i[1]] =  sorted(nodenext[i[1]])\n",
    "\n",
    "khat = 0\n",
    "for i in nodenext:\n",
    "    khat+= len(i)\n",
    "khat = khat / N\n",
    "\n",
    "a = 0\n",
    "for i in range(N):\n",
    "    a+=sum(distance[i,nodenext[i]])\n",
    "a = alpha * a / (khat*N)\n",
    "\n",
    "n = 2\n",
    "J = (1 / khat) * np.exp(-( (n-1)/n ) * ( distance / a)**n)\n",
    "\n",
    "\n",
    "f_=0.5\n",
    "\n",
    "\n",
    "S=np.ones(N, dtype=int)\n",
    "\n",
    "results = Parallel(n_jobs=5)(delayed( runz )(S,f_,mcmc,nodenext,J,T[y],q,K) for y in range(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 17,  7, 10, 10, 13, 16,  0,  8, 14, 10,  2, 12, 11,  1,  6,  5,\n",
       "       14,  2,  1, 16,  4,  8,  8, 16,  0,  8,  6, 13,  1,  9, 15,  5, 17,\n",
       "        6,  6, 18, 11, 18,  6, 13,  7,  4,  8,  8,  6,  8, 11,  3, 19, 13,\n",
       "       10,  2, 19, 10,  8, 19,  4,  4,  0, 16, 10, 19,  8, 12, 11,  4,  7,\n",
       "       11,  6, 17,  6, 13, 15, 17, 11,  0,  7, 13,  0, 14, 12, 12,  1, 12,\n",
       "       15, 16,  7, 15,  8,  3, 16,  2, 13, 12,  7, 11, 11,  5,  9,  1, 15,\n",
       "        5,  0, 17,  7, 16,  3, 13, 16, 11,  0,  9,  8, 18,  4,  3,  9, 17,\n",
       "       16,  6,  6, 17, 17, 13,  1,  9,  6,  7, 18, 18,  1,  3, 11, 13, 16,\n",
       "        8,  4, 12,  9,  2, 12,  9,  9, 12,  4,  0, 17, 18,  6, 12, 16,  1,\n",
       "       17,  6, 13,  4,  9,  1,  2,  2, 11,  8, 16, 17,  4, 17,  1,  8,  1,\n",
       "       16,  9, 16, 17, 13, 16,  9, 16, 19,  9,  4, 12, 18,  1,  1, 16, 13,\n",
       "       13, 11,  4,  4,  6, 12, 17,  5,  6, 17, 12, 18, 18,  2, 17,  1, 11,\n",
       "        9,  6, 16, 11, 15,  1,  5,  8,  1, 12, 14, 11, 18,  8, 11, 11, 17,\n",
       "        2, 14, 14,  2, 14, 11,  1,  8, 11,  8, 12,  6,  5,  2, 10,  4,  8,\n",
       "       14, 17, 12, 15, 17,  8, 13, 12, 13,  4,  7,  7,  8, 10,  4,  9, 11,\n",
       "        4,  7, 16, 12,  9,  5, 16,  6, 10, 17,  0,  4, 10,  7, 17,  6,  6,\n",
       "        3, 11, 17,  6,  4, 15, 11,  8, 15, 14, 18, 13, 13,  3,  8, 12,  9,\n",
       "        4,  3,  8,  8,  1, 12,  4,  6, 14,  1,  3, 12, 16, 18, 18,  8,  1,\n",
       "       14, 10, 15, 17, 11, 15, 11,  5,  9,  8,  2,  9, 16, 11, 13,  0, 14,\n",
       "        0, 13, 14,  3,  6, 13,  8, 14,  6, 16,  7,  7, 18, 13, 18, 13,  4,\n",
       "        2,  7,  8, 15,  4, 12,  4,  9, 19, 18,  1,  8, 18, 17, 19, 17, 18,\n",
       "       16,  0, 19,  2, 18,  2,  1,  8, 10, 18,  1, 11, 18, 14,  3, 11,  5,\n",
       "       12, 10,  8,  7, 16,  8, 18,  2,  9, 12, 17,  7, 11, 18, 16, 15, 15,\n",
       "        3,  8, 17,  4,  1,  0, 19,  4,  9, 17, 14,  4,  2, 11,  7, 15,  8,\n",
       "       18,  3,  2, 16,  9,  8,  6, 10,  2,  6, 19,  5, 18, 10,  0, 18,  4,\n",
       "       11, 12,  8,  0, 11, 19, 19,  9, 10, 15, 17, 14, 13, 12,  3, 18,  2,\n",
       "        4, 18,  9, 12, 18,  4,  1, 12, 10, 19, 10, 16, 13, 12,  4, 12,  4,\n",
       "        0, 19,  1,  8,  9, 10,  6, 17,  5,  8, 13, 12, 13, 15, 16,  8,  1,\n",
       "        1, 19, 11,  4,  6, 11, 15,  9,  0,  8,  9,  0,  3,  9, 10, 18,  6,\n",
       "        9,  9,  6,  6, 18,  3,  3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[59][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
